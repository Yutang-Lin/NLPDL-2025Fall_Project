import os
import google.generativeai as genai
import json
import time
from tqdm import tqdm  # ç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡

# ================= é…ç½®åŒºåŸŸ =================
# è¯·æ›¿æ¢ä½ çš„ API Key
os.environ["GOOGLE_API_KEY"] = "AIzaSyAttTS3ZPtU4_wE-3wnOUcIquatiyhFlx4"
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])

# åˆå§‹åŒ–æ¨¡å‹ (æ¨èä½¿ç”¨ Pro æˆ– Flash)
model = genai.GenerativeModel('gemini-3-flash-preview')

# å®šä¹‰ 10 ä¸ªé«˜éš¾åº¦é¢†åŸŸ
FIELDS = [
    "Physics",
    "Mathematics",
    "Computer Science",
    "Philosophy",
    "Geography",
    "Chemistry",
    "Biology",
    "Economics",
    "Statistics",
    "Law"
]

# æ¯ä¸ªé¢†åŸŸç”Ÿæˆå¤šå°‘æ¡
TARGET_PER_FIELD = 50
# æ¯æ¬¡ API è°ƒç”¨ç”Ÿæˆå¤šå°‘æ¡ (å»ºè®® 5 æ¡ï¼Œä¿è¯æ¯æ¡çš„æ·±åº¦å’Œé•¿åº¦)
BATCH_SIZE = 5

# ================= æ ¸å¿ƒç”Ÿæˆé€»è¾‘ =================

def generate_expert_dataset():
    all_data = []
    
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆæ•°æ®ä»»åŠ¡")
    print(f"ğŸ“š é¢†åŸŸæ•°é‡: {len(FIELDS)}")
    print(f"ğŸ¯ ç›®æ ‡æ€»æ•°: {len(FIELDS) * TARGET_PER_FIELD}")
    
    # éå†æ¯ä¸ªé¢†åŸŸ
    for field in FIELDS:
        print(f"\nProcessing Field: {field}...")
        field_data = []
        
        # è®¡ç®—éœ€è¦å¤šå°‘ä¸ªæ‰¹æ¬¡ (ä¾‹å¦‚ 50 / 5 = 10 æ¬¡)
        num_batches = TARGET_PER_FIELD // BATCH_SIZE
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºå½“å‰é¢†åŸŸçš„è¿›åº¦
        for i in tqdm(range(num_batches), desc=f"Generating {field}"):
            
            # ç²¾å¿ƒè®¾è®¡çš„ Promptï¼Œå¼ºè°ƒ "Graduate-level" å’Œ "English"
            prompt = f"""
            Role: You are a distinguished professor specializing in {field}.
            
            Task: Generate {BATCH_SIZE} distinct Q&A pairs focusing on the **most important, foundational, and cornerstone concepts** of this field.
            
            Guidelines:
            1.  **Selection Criteria:** Do NOT ask obscure trivia or insanely difficult calculations. Instead, ask about the "Big Ideas", "Central Dogmas", or "Fundamental Theorems" that define the field.
            2.  **Question Style:** The questions should ask for conceptual depth, mechanisms, or the underlying logic (e.g., "Why does...", "Explain the principle of...", "What is the significance of...").
            3.  **Answer Quality:** The answer must be rigorous, academic, and comprehensive (Graduate-level understanding), NOT a simplified summary.
            4.  **Language:** Strictly English.
            5.  **Format:** Return a RAW JSON list.
            
            Output Example (if field was Biology):
            [
                {{
                    "field": "Biology",
                    "question": "Explain the central dogma of molecular biology and its significance in genetic expression.",
                    "answer": "The central dogma describes the flow of genetic information within a biological system..."
                }}
            ]
            """
            
            try:
                # è°ƒç”¨ Gemini API
                response = model.generate_content(
                    prompt,
                    generation_config={
                        "response_mime_type": "application/json", # å¼ºåˆ¶ JSON
                        "temperature": 0.8 #ç¨å¾®é«˜ä¸€ç‚¹ï¼Œå¢åŠ å¤šæ ·æ€§
                    }
                )
                
                # è§£ææ•°æ®
                batch_items = json.loads(response.text)
                
                # ç®€å•éªŒè¯ä¸€ä¸‹æ•°é‡
                if isinstance(batch_items, list):
                    field_data.extend(batch_items)
                
                # é¿å…è§¦å‘ API é€Ÿç‡é™åˆ¶ (Rate Limit)
                time.sleep(2)
                
            except Exception as e:
                print(f"âš ï¸ Error in batch {i} for {field}: {e}")
                time.sleep(5) # å‡ºé”™å¤šæ­‡ä¼šå„¿

        # å°†å½“å‰é¢†åŸŸçš„æ•°æ®åŠ å…¥æ€»è¡¨
        all_data.extend(field_data)
        
        # ä¸ºäº†é˜²æ­¢ç¨‹åºåŠé€”å´©æºƒï¼Œæ¯å®Œæˆä¸€ä¸ªé¢†åŸŸå°±å­˜ä¸€æ¬¡ç›˜
        save_to_file(all_data, filename="dataset_capability_english_partial.json")

    # æœ€ç»ˆä¿å­˜
    save_to_file(all_data, filename="dataset_capability_english_final.json")
    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼å…±ç”Ÿæˆ {len(all_data)} æ¡æ•°æ®ã€‚")

def save_to_file(data, filename):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    generate_expert_dataset()